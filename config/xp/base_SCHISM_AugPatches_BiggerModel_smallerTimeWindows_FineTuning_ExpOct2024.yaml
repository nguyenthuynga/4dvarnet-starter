# @package _global_
#change patch (line 5-15, choose one the three options), trainer vs tester (base_test line 88-89) and ckpt_path (line 93, comment it if you use training phase)
#change #training epochs, period of train/val/test

#changes in Bigger Model:  n_step=15 (instead of 10), dim_hidden=64 (instead of 32) in prior cost, dim_hidden=96 (instead of 48) in gradmod, bilin_quad: False (instead of True), and downsamp: 2 (instead of None)
#reduce time windows (from 15 to 10), then I reduce again to 8 (instead of 10)

#IMPORTANT!!!!  for finetuning: 
# 1, I use lr: 0.00015 instead of 0.0003 (devided by 2);  
# 2. data module is data.py for training/finetuning on OSE and data_DWS_OSSE_realgapMask_and_50percentArtificialPatches for OSSE
# 3. path is OSE instead of OSSE, test period changed too
# 4. Load pretrained weight from pretraining OSSE
# 5. save weight more often (every 2 epochs) and train less (around 50 epochs)
paths:
    # #for training
    # GT: data/data_cropped_SCHISM_Nga.nc
    # # patch: data/Obs_patch_SCHISM_Nga.nc
    # patch: data/Obs_patch_SCHISM_Nga_0084percentageCloud.nc

    # #testing with OSE, change period of test
    GT: data/Obs_SPM_log10_aNam.nc
    patch: data/Obs_SPM_log10_aNam_removed_50percent_patch_again.nc


trainer:
  _target_: pytorch_lightning.Trainer
  inference_mode: False
  gradient_clip_val: 0.5
  accelerator: gpu
  devices: 1
  logger: 
    _target_: pytorch_lightning.loggers.CSVLogger
    save_dir: ${hydra:runtime.output_dir}
    name: ${hydra:runtime.choices.xp}
    version: 'IncreaseLr1em4'
  max_epochs: 227
  callbacks:
    - _target_: src.versioning_cb.VersioningCallback
    - _target_: pytorch_lightning.callbacks.LearningRateMonitor
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      monitor: val_mse
      save_top_k: 30
      filename: '{val_mse:.4f}-{epoch:03d}'

datamodule:
  _target_: src.data.BaseDataModule
  input_da: 
    _target_: src.utils.load_bbp_data
    path1: ${paths.GT}
    path2: ${paths.patch}
  domains:
    train:
      time: {_target_: builtins.slice, _args_: ['2020-02-01', '2020-06-15']}
    val: 
      time: {_target_: builtins.slice, _args_: ['2020-06-16', '2020-06-30']}
    test: 
      time: {_target_: builtins.slice,  _args_: ['2020-02-01', '2020-11-26']}
  xrds_kw:
    patch_dims: { time: 10, lat: 240, lon: 300}
    strides: { time: 1, lat: 240, lon: 300}
  dl_kw: {batch_size: 4, num_workers: 1}
  aug_factor: 1
  aug_only: True

model:
  _target_: src.models.Lit4dVarNet
  opt_fn:
    _target_: src.utils.cosanneal_lr_adam
    _partial_: true
    lr: 0.0001 # Updated from 1e-3
    T_max: 175
  rec_weight:
      _target_: src.utils.get_triang_time_wei
      patch_dims: ${datamodule.xrds_kw.patch_dims}
      crop: {time: 0, lat: 0, lon: 0}
      offset: 1
  solver: 
    _target_: src.models.GradSolver
    n_step: 15
    lr_grad: 0.2
    prior_cost: 
      _target_: src.models.BilinAEPriorCost
      dim_in: ${datamodule.xrds_kw.patch_dims.time}
      dim_hidden: 64
      downsamp: 2
      bilin_quad: False

    obs_cost: 
      _target_: src.models.BaseObsCost
    grad_mod: 
      _target_: src.models.ConvLstmGradModel
      dim_in: ${datamodule.xrds_kw.patch_dims.time}
      dim_hidden: 96


entrypoints:
  - _target_: pytorch_lightning.seed_everything
    seed: 333
  - _target_: src.train.base_training
  # - _target_: src.test.base_test
    trainer: ${trainer}
    lit_mod: ${model}
    dm: ${datamodule}
    ckpt_path: "/homes/n23nguye/4dvarnet-starter/outputs/2024-03-12/09-50-58/base_SCHISM_AugPatches_BiggerModel_smallerTimeWindows/TimeWindows10/checkpoints/val_mse=24.5515-epoch=197.ckpt"
